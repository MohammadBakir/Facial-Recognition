{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras - Facial Recognition\n",
    "\n",
    "### Problem Description\n",
    "Implement a Deep Learning Model that recognizes faces of individual to allow access entry to a location only if individual is happy\n",
    "\n",
    "The \"Happy\" rule is strictly applied, the algorithm will use pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy. \n",
    "\n",
    "You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled. \n",
    "\n",
    "<img src=\"images/house-members.png\" style=\"width:550px;height:250px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Packages\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load Datasets\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "#Normalize image vectors, dividing by 225 the range can be described with a 0.0-1.0 where 0.0 means (0x00)\n",
    "#and 1.0 means 255(0XFF)\n",
    "X_train = X_train_orig/255\n",
    "X_test = X_test_orig/255\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "* Images are of shape (64,64,3)\n",
    "* Training Set: 600 Pictures\n",
    "* Test Set: 150 Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Happy Model Built On Keras Framework\n",
    "\n",
    "def Happy_Facial_Recognition(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Happy Model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    #Instantiate a Keras tensor\n",
    "    X_input =Input(input_shape)\n",
    "    \n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    #Padding produces an output that has the same length of the original input. Padding prevents our image from \n",
    "    #getting smaller with each convolutional layer\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    \n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    \n",
    "    \"\"\"\n",
    "    Batch normalization reduces the amount by which the hidden unit values shift around.It's a form of slight\n",
    "    regularization effects and adds some noise to each hidden layer's activations. To increase stability of a\n",
    "    neural network, batch normalization normalizes the output of a previous activation layer by subtracting the\n",
    "    batch mean and dividing by the batch standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    \"\"\"Max pooling dis utilized to down-sample the features (such as edges) that were just extracted which gives \n",
    "    a more approximate representation of where the other features are, making the network more generalizable.\"\"\"\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "    \n",
    "    \n",
    "    #FLATTEN X\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "    \n",
    "    #Generate model instance\n",
    "    model = Model(inputs = )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
