{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras - Facial Recognition \n",
    "\n",
    "### Problem Description\n",
    "Implement a Deep Learning Model that recognizes facial feature of an individual and allows them access entry to location only if the individual is happy.\n",
    "\n",
    "The \"Happy\" rule is strictly applied, the algorithm will use pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy. \n",
    "\n",
    "You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled. \n",
    "\n",
    "<img src=\"images/house-members.png\" style=\"width:550px;height:250px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#Import Required Packages\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import backend as Kend\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from kt_utils import *\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load Datasets\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "#Normalize image vectors, dividing by 225 the range can be described with a 0.0-1.0 where 0.0 means (0x00)\n",
    "#and 1.0 means 255(0XFF)\n",
    "X_train = X_train_orig/255\n",
    "X_test = X_test_orig/255\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "* Images are of shape (64,64,3)\n",
    "* Training Set: 600 Pictures\n",
    "* Test Set: 150 Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Happy Model Built On Keras Framework\n",
    "\n",
    "def Happy_Facial_Recognition(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Happy Model\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    #Instantiate a Keras tensor\n",
    "    X_input =Input(input_shape)\n",
    "    \n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    #Padding produces an output that has the same length of the original input. Padding prevents our image from \n",
    "    #getting smaller with each convolutional layer\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    \n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    \n",
    "    \"\"\"\n",
    "    Batch normalization reduces the amount by which the hidden unit values shift around.It's a form of slight\n",
    "    regularization effects and adds some noise to each hidden layer's activations. To increase stability of a\n",
    "    neural network, batch normalization normalizes the output of a previous activation layer by subtracting the\n",
    "    batch mean and dividing by the batch standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    \"\"\"Max pooling dis utilized to down-sample the features (such as edges) that were just extracted which gives \n",
    "    a more approximate representation of where the other features are, making the network more generalizable.\"\"\"\n",
    "    X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "    \n",
    "    X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = MaxPooling2D((2, 2), name='max_pool2')(X)\n",
    "    \n",
    "    #FLATTEN X\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "    \n",
    "    #Generate model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name='Happy_Facial_Recognition')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 7201      \n",
      "=================================================================\n",
      "Total params: 21,441\n",
      "Trainable params: 21,313\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print Summary of Model\n",
    "facial_recog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Model\n",
    "facial_recog = Happy_Facial_Recognition(X_train.shape[1::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')#Compile Model\n",
    "facial_recog.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Done Training\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "facial_recog.fit(x = X_train, y = Y_train, epochs=40, batch_size=16, verbose=0)\n",
    "print(\"Model Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n",
      "\n",
      "Loss = 0.02397265563408534\n",
      "Test Accuracy = 0.9733333309491475\n"
     ]
    }
   ],
   "source": [
    "#Test trained model against test data\n",
    "preds = facial_recog.evaluate(X_test, Y_test)\n",
    "\n",
    "print()\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "#Test with own image. 0 is unhappy 1 is happy\n",
    "img_path = 'images/my_image.png'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print(facial_recog.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
